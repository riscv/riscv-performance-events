[[body]]
== Groups

All the events and metrics are split between several groups which are described in details sub-sections below.
In addition most of the groups are further split into 2 variants:

* RET - for non-speculative events counted at retirement. For example, INST.RET counts retired instructions.
* SPEC - for speculative events. For example, INST.SPEC counts instructions that are issued to the backend pipeline, regardless of whether they retire.

NOTE: _In general, RET events are more useful for performance analysis, since they are consistent with software's view of the instruction flow. But they can be significantly more expensive to implement, as they require event data to be staged along with the associated instruction to retirement. It is up to implementations to decide whether to support the RET, SPEC, or both variants of an event._

=== GENERAL

This group contains general events not specific to a particular part of CPU pipeline. 

include::adoc_event_tables/general.adoc[]

=== CTRL_FLOW (retirement)

Retirement control flow group which contains events and metrics for counting all control transfer instructions, or just those that were mispredicted, including breakdowns by transfer type.

include::adoc_event_tables/prediction_retired.adoc[]

include::adoc_event_tables/prediction_metrics.adoc[]

=== CACHE

This group contains events and metrics for instruction, data, and unified caches.  Events in this group count accesses from the cache's perspective; if an instruction performs two cache accesses, they will be counted separately.  Further, all events in this group count speculative accesses.

The top level of the CACHE event hierarchy idenfies the cache associated with the event.  The standard caches are enumerated in the table below.  An implementation is expected to support only the events associated with caches implemented in the design.

.CACHE Event Caches
[cols="20%,80%"]
|===
| L1D | Level 1 data cache
| L1I | Level 1 instruction cache
| L2 | Level 2 cache
| L3 | Level 3 cache
| LL | Last level cache
|===

NOTE: The events for the last level cache may be aliased to the events for one of the cache levels listed above it.

NOTE: Some of the caches listed above may reside outside the processor core.  In such cases, associated events may not be able to be counted by the core PMU.

The second level of the CACHE event hierarchy identifies the access or operation type, as shown in the table below.

.CACHE Event Access or Operation Type
[cols="20%,80%"]
|===
| LOAD | Data reads.  This includes explicit accesses (e.g., LW instructions) and implicit accesses (e.g., page table walks).  If PREF_I and CODE access types below are implemented, it is recommended that LOAD not include prefetch or code fetch accesses.
| STORE | Data writes.  This includes explicit accesses (e.g., SW instructions) and implicit accesses (e.g., page accessed/dirty bit updates).
| RFO | Data reads-for-ownership.  These are a subset of the LOAD type, counting only those reads that require data returned in exclusive state, as part of a read-modify-write operation.
| PREF_I | Incoming prefetch reads.
| CODE | Code fetch reads.
| PREF_O | Outbound prefetches, for the purpose of pulling lines into the associated cache.
| SNOOP | Cache coherency snoops.
| REFILL | Incidences where a cache miss results in data from memory or an outer cache filling a line in the cache.
| WB | Modified lines written out from the cache to memory or to an outer cache.
|===

Some events in the CACHE event hierarchy include additional levels.  For cache lookup access types (LOAD, STORE, RFO, PREF_I, and CODE), a third level counts per lookup result.

.CACHE Event Lookup Results
[cols="20%,80%"]
|===
| ACCESS | Cache lookups
| HIT    | Cache hits
| MISS   | Cache misses
| MERGE  | Lookups that merged with an outstanding lookup
|===

For the LOAD type, there is additionally a MISS.CYCLES event that counts core cycles (at the rate of the Zicntr cycle counter) while a load miss is outstanding.

The SNOOP type events have two additional levels of hierarchy.

.CACHE Event Snoop Types
[cols="20%,80%"]
|===
| LOCAL  | Snoops from accesses local to the hart
| REMOTE | Snoops from remote harts
|===

.CACHE Event Snoop Results
[cols="20%,80%"]
|===
| ACCESS | Snoop lookups
| HIT    | Snoops that hit unmodified data
| HITM   | Snoops that hit modified data
| MISS   | Snoops that miss
|===

The PREF_O type events have an additional level of hierarchy.
[cols="20%,80%"]
|===
| ISSUED | Prefetch issued
| UNUSED | Line was prefetched into the cache, but was invalidated or replaced before it was accessed
|===

The full set of CACHE events and metrics is shown below.

include::adoc_event_tables/cache.adoc[]

include::adoc_event_tables/cache_metrics.adoc[]

=== TLB (retirement)

This group contains events and metrics for data and instruction TLB caches (all levels) counted at retirement.

include::adoc_event_tables/tlb_retired.adoc[]

include::adoc_event_tables/tlb_retired_metrics.adoc[]

=== TLB (speculative)

This group contains events and metrics for data and instruction TLB caches (all levels) counted speculatively.

include::adoc_event_tables/tlb_spec.adoc[]

include::adoc_event_tables/tlb_spec_metrics.adoc[]

=== TOP-DOWN

This group contains events and metrics related for Top-down Microarchitecture Analysis (TMA) methodology.

TMA is an industry-standard methodology https://ieeexplore.ieee.org/document/6844459[introduced by Intel] in characterizing the performance of SPEC CPU2006 on Intel CPUs, and since used to characterize https://www.mdpi.com/2078-2489/14/10/554[HPC workloads], https://ieeexplore.ieee.org/abstract/document/9820717[GPU workloads], https://dl.acm.org/doi/10.1145/3369383[microarchitecture changes], https://ieeexplore.ieee.org/abstract/document/9579960[pre-silicon performance validation failures], and more.

TMA allows even developers with minimal microarchitecture knowledge to understand, for a given workload, where bottlenecks reside.  It does so by accounting for the utilization of each pipeline "slot" in the microarchitecture.  As an example, for a 4-wide implementation, there are 4 slots to account for each cycle.  When the hardware is utilized with optimal efficiency, each slot is occupied by an instruction or micro-operation (uop) that will go on to execute and retire.  When bottlenecks occur, due perhaps to a cache miss, branch misprediction, or any number of other microarchitectural conditions, some slots may be either unused or thrown away, which results in inefficiency and reduced performance.  TMA is able to identify these wasted slots, and the stalls, clears, misses, or other events that cause them.  This enables developers to make informed decisions when tuning their code.

TMA accomplishes this by defining a set of hierarchical states into which each slot is categorized.  Each cycle, the frontend of the processor (responsible for instruction fetch and decode) can issue some implementation-defined number (_N_) of instructions/uops to the backend (instruction execution and retire).  Hence there are _N_ issue slots to be categorized per cycle.  At the top level of the TMA hierarchy, issue slots are categorized as described below.

[align="center"]
.Topdown Level 1
image::images/tma-l1.svg[TMA Level 1]

* Frontend Bound - The frontend did not issue a uop to the backend for execution.  Example causes include stalls that result from cache or TLB misses during instruction fetch.
* Backend Bound - The backend could not consume a uop from the frontend.  Example causes include backpressure that results from cache or TLB misses on data (load/store) accesses, or from oversubscribed execution units.
* Bad Speculation - The uop was dropped, as a result of a pipeline clear.  Example clears include branch/jump mispredictions, or memory ordering clears.  This category also includes any pipeline clear recovery cycles during which issue slots go unfilled.
* Retiring - The uop retired.  Ideally the majority of slots fall into this state.

Many of the top-level states listed above include further breakdown at the 2nd and 3rd levels of the TMA hierarchy, as illustrated below.  

[align="center"]
.Topdown Hierarchy
image::images/tma-full.svg[TMA Hierarchy]

[NOTE]
====
_Some imprecision within the event hierarchy is allowed and even expected.  The standard L2 and L3 events may not sum precisely to the parent L1 or L2 events, respectively, as it is expected that there will be some additional sources of bottlenecks beyond those represented by the standard events.  The exception is the Backend Bound L2 events (Core Bound and Memory Bound), which ideally should sum to the Backend Bound event total._

_Because of this possible imprecision, it is recommended that lower level TMA events are examined only when the parent event count or rate is higher than expected.  This avoids spending time on misleading L2 or L3 events that may be implemented by imprecise event formulas rather than precise hardware events._

_Implementations may opt to add custom L2 or L3 events, to identify additional bottlenecks specific to the microarchitecture._
====

The events which follow count slots for each of the states listed above, while the metrics express the slots per state value as a percentage of total slots.

include::adoc_event_tables/topdown.adoc[]

include::adoc_event_tables/topdown_metrics.adoc[]

=== RVV (retirement)

This group contains events and metrics related to vectorized operations counted at retirement.

include::adoc_event_tables/rvv_retired.adoc[]

include::adoc_event_tables/rvv_retired_metrics.adoc[]

=== RVV (speculative)

This group contains events and metrics related to vectorized operations counted speculatively.

include::adoc_event_tables/rvv_spec.adoc[]

include::adoc_event_tables/rvv_spec_metrics.adoc[]
